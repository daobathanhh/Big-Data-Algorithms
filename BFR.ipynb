{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc6e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sc:\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564d98ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.submitTime', '1765458003968'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.port', '33537'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://name:8088/proxy/application_1765452215483_0004'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'name'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/python/:/usr/local/spark/python:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.7-src.zip'),\n",
       " ('spark.ui.port', '4050'),\n",
       " ('spark.app.id', 'application_1765452215483_0004'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.appUIAddress', 'http://edge:4050'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1765452215483_0003'),\n",
       " ('spark.app.startTime', '1765458028258'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.jars', ''),\n",
       " ('spark.driver.host', 'edge'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.driver.bindAddress', '192.1.1.70'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', '22125094_lab6')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"22125094_lab6\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_data = sc.textFile(\"hdfs://name:9000/Lab6_Datasets/online_retail.csv\")\n",
    "\n",
    "header = retail_data.first()\n",
    "retail_data = retail_data.filter(lambda x : x != header)\n",
    "retail_data = retail_data.map(lambda x : x.split(\",\"))  \n",
    "\n",
    "retail_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(x):\n",
    "    try:\n",
    "        for i in x:\n",
    "            if i == \"\":\n",
    "                return None\n",
    "        return (\n",
    "            int(x[0]),\n",
    "            x[1],\n",
    "            x[2],\n",
    "            x[3],\n",
    "            int(x[4]),\n",
    "            x[5],\n",
    "            float(x[6]),\n",
    "            x[7],\n",
    "            x[8]\n",
    "        )\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "retail_parsed = retail_data.map(parse_row).filter(lambda x: x is not None)\n",
    "retail_parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af571347",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = retail_parsed.map(lambda x: (x[7], x[1], x[5], x[4] * x[6]))\n",
    "rfm.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_date(x):\n",
    "    cus_id, inv_no, inv_date, total = x\n",
    "    inv_dt = datetime.strptime(inv_date, \"%m/%d/%Y %H:%M\")\n",
    "    return (cus_id, inv_no, inv_dt, total)\n",
    "\n",
    "rfm = rfm.map(parse_date)\n",
    "rfm.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15e2dc3",
   "metadata": {},
   "source": [
    "Compute RFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0668f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_buy = rfm.map(lambda x: (x[0], x[2])).reduceByKey(lambda a, b : max(a, b))\n",
    "latest = last_buy.map(lambda x: x[1]).max()\n",
    "last_buy.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82395ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "recency = last_buy.map(lambda x: (x[0], (latest - x[1]).days))\n",
    "recency.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = rfm.map(lambda x: (x[0], x[1])).distinct()\n",
    "frequency = frequency.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b : a + b)\n",
    "frequency.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e80970",
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = rfm.map(lambda x: (x[0], x[3])).reduceByKey(lambda a, b : a + b)\n",
    "monetary.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08165431",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_rdd = recency.join(frequency).join(monetary)\n",
    "rfm_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_final = rfm_rdd.map(lambda x: (float(x[1][0][0]), float(x[1][0][1]), float(x[1][1]))).collect()\n",
    "rfm_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac6908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.75536054,  1.06406227, -0.1609506 ],\n",
       "       [-0.89441086,  1.49277746,  0.30825616],\n",
       "       [-0.09983759, -0.2220833 , -0.011134  ],\n",
       "       ...,\n",
       "       [ 0.58548186, -0.3292621 , -0.19704634],\n",
       "       [ 0.62521053, -0.43644089, -0.19312035],\n",
       "       [ 0.52588887, -0.43644089, -0.21215216]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "rfm_array = np.array(rfm_final)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_array)\n",
    "rfm_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac35db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n, d = rfm_scaled.shape\n",
    "\n",
    "ALPHA_THRESHOLD = 2.0\n",
    "EUCLIDEAN_THRESHOLD = 2.0\n",
    "MERGE_THRESHOLD = 2.0\n",
    "\n",
    "# DS = []\n",
    "# CS = []\n",
    "# RS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "78ee307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterSummary:\n",
    "    def __init__(self, d):\n",
    "        self.N = 0\n",
    "        self.SUM = np.zeros(d)\n",
    "        self.SUMSQ = np.zeros(d)\n",
    "\n",
    "    def add_point(self, point):\n",
    "        self.N += 1\n",
    "        self.SUM += point\n",
    "        self.SUMSQ += point ** 2\n",
    "        \n",
    "    def add_points(self, points):\n",
    "        points = np.array(points)\n",
    "        self.N += len(points)\n",
    "        self.SUM += np.sum(points, axis=0)\n",
    "        self.SUMSQ += np.sum(points ** 2, axis=0)\n",
    "        \n",
    "    def merge(self, other_summary):\n",
    "        self.N += other_summary.N\n",
    "        self.SUM += other_summary.SUM\n",
    "        self.SUMSQ += other_summary.SUMSQ\n",
    "        \n",
    "    def get_centroid(self):\n",
    "        if self.N == 0: \n",
    "            return np.zeros(d)\n",
    "        return self.SUM / self.N\n",
    "        \n",
    "    def get_variance(self):\n",
    "        if self.N == 0: \n",
    "            return np.zeros(d)\n",
    "        return (self.SUMSQ / self.N) - (self.SUM / self.N) ** 2\n",
    "        \n",
    "    def get_std(self):\n",
    "        return np.sqrt(np.abs(self.get_variance()))\n",
    "    \n",
    "def mahalanobis_dist(point, centroid, std):\n",
    "    if np.any(std < 1e-6):\n",
    "        # return np.linalg.norm(point - centroid)\n",
    "        std = std + 1e-6\n",
    "    return np.sqrt(np.sum(((point - centroid) / std) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68583876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sets(first_chunk, k):\n",
    "    n, d = first_chunk.shape\n",
    "    num_clusters = k\n",
    "    initial_centroids, labels = KMeans(first_chunk, num_clusters)\n",
    "    DS = []\n",
    "    CS = []\n",
    "    RS = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_points = first_chunk[labels == i]\n",
    "        if len(cluster_points) >= 1: \n",
    "            cs = ClusterSummary(d)\n",
    "            cs.add_points(cluster_points) \n",
    "            DS.append(cs)\n",
    "    return DS, CS, RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "78533052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_merge(c1, c2, d):\n",
    "    std1 = c1.get_std()\n",
    "    std2 = c2.get_std()\n",
    "    merged = ClusterSummary(d)\n",
    "    merged.merge(c1)\n",
    "    merged.merge(c2)\n",
    "\n",
    "    std_merged = merged.get_std()\n",
    "    centroid1 = c1.get_centroid()\n",
    "    centroid2 = c2.get_centroid()\n",
    "    threshold = MERGE_THRESHOLD * np.sqrt(d)\n",
    "    dist = mahalanobis_dist(centroid1, centroid2, std_merged)\n",
    "    return dist < threshold\n",
    "    # return np.all(std_merged <= (std1 + std2) * MERGE_THRESHOLD)\n",
    "\n",
    "def merge_CS_clusters(CS, d):\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        i = 0\n",
    "        while i < len(CS):\n",
    "            j = i + 1\n",
    "            while j < len(CS):\n",
    "                if can_merge(CS[i], CS[j], d):\n",
    "                    CS[i].merge(CS[j])\n",
    "                    CS.pop(j)\n",
    "                    merged = True\n",
    "                else:\n",
    "                    j += 1\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c0548679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_remaining_chunks(DS, CS, RS, chunks, d):\n",
    "    for i, chunk in enumerate(chunks[1:]):\n",
    "        merge_CS_clusters(CS, d)\n",
    "        print(f\"Processing Chunk {i+2} ({len(chunk)} points)...\")\n",
    "        for point in chunk:\n",
    "            # DS\n",
    "            min_dist = float('inf')\n",
    "            best_ds = -1\n",
    "            for idx, cluster in enumerate(DS):\n",
    "                dist = mahalanobis_dist(point, cluster.get_centroid(), cluster.get_std())\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_ds = idx\n",
    "            if min_dist < ALPHA_THRESHOLD * np.sqrt(d):\n",
    "                DS[best_ds].add_point(point)\n",
    "                continue\n",
    "            # CS\n",
    "            min_dist = float('inf')\n",
    "            best_cs = -1\n",
    "            for idx, cluster in enumerate(CS):\n",
    "                dist = mahalanobis_dist(point, cluster.get_centroid(), cluster.get_std())\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    best_cs = idx\n",
    "            if min_dist < ALPHA_THRESHOLD * np.sqrt(d):\n",
    "                CS[best_cs].add_point(point)\n",
    "                continue\n",
    "            \n",
    "            formed = False\n",
    "            for r_idx, r in enumerate(RS):\n",
    "                dist = np.linalg.norm(point - r)\n",
    "                if dist < EUCLIDEAN_THRESHOLD:\n",
    "                    cs = ClusterSummary(d)\n",
    "                    cs.add_point(point)\n",
    "                    cs.add_point(r)\n",
    "                    CS.append(cs)\n",
    "                    RS.pop(r_idx)\n",
    "                    formed = True\n",
    "                    break\n",
    "\n",
    "            if formed:\n",
    "                continue\n",
    "            # RS\n",
    "            RS.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "976dec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_merge(DS, CS, RS, d):\n",
    "    print(\"Finalizing...\")\n",
    "    # CS into DS\n",
    "    merge_CS_clusters(CS, d)\n",
    "    for cs in CS:\n",
    "        min_dist = float('inf')\n",
    "        best_idx = -1\n",
    "        for idx, ds in enumerate(DS):\n",
    "            dist = mahalanobis_dist(cs.get_centroid(), ds.get_centroid(), ds.get_std())\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_idx = idx\n",
    "        # force merge to nearest DS\n",
    "        if best_idx != -1:\n",
    "            DS[best_idx].merge(cs)\n",
    "\n",
    "    for point in RS:\n",
    "        min_dist = float('inf')\n",
    "        best_idx = -1\n",
    "        for idx, ds in enumerate(DS):\n",
    "            dist = mahalanobis_dist(point, ds.get_centroid(), ds.get_std())\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                best_idx = idx\n",
    "        if best_idx != -1:\n",
    "            DS[best_idx].add_point(point)\n",
    "    CS.clear()\n",
    "    RS.clear()\n",
    "    print(\"Clustering Complete.\")\n",
    "    for i, ds in enumerate(DS):\n",
    "        print(f\"Cluster {i}: N={ds.N} Centroid={ds.get_centroid()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b07b331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_points = len(rfm_scaled)\n",
    "chunk_size = int(total_points / 10) # 10 chunks \n",
    "chunks = [rfm_scaled[i:i + chunk_size] for i in range(0, total_points, chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "275748ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bfr_wcss(data, DS):\n",
    "    wcss = 0.0\n",
    "    for p in data:\n",
    "        min_dist = float(\"inf\")\n",
    "        for ds in DS:\n",
    "            centroid = ds.get_centroid()\n",
    "            dist = np.linalg.norm(p - centroid) ** 2\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        wcss += min_dist\n",
    "    return wcss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a10de801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bfr(chunks, k):\n",
    "    first_chunk = chunks[0]\n",
    "    _, d = chunks[0].shape\n",
    "\n",
    "    DS, CS, RS = init_sets(first_chunk, k)\n",
    "\n",
    "    print(\"Initialization done.\")\n",
    "    print(\"DS:\", len(DS), \"CS:\", len(CS), \"RS:\", len(RS))\n",
    "\n",
    "    process_remaining_chunks(DS, CS, RS, chunks, d)\n",
    "\n",
    "    print(\"Streaming processing done.\")\n",
    "    print(\"DS:\", len(DS), \"CS:\", len(CS), \"RS:\", len(RS))\n",
    "\n",
    "    final_merge(DS, CS, RS, d)\n",
    "\n",
    "    print(\"Final merge done.\")\n",
    "    print(\"DS:\", len(DS), \"CS:\", len(CS), \"RS:\", len(RS))\n",
    "    return DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f2be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DS = run_bfr(chunks, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
